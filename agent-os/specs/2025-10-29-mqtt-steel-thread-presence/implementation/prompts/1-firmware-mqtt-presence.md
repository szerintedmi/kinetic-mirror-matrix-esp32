We're continuing our implementation of MQTT Steel Thread: Presence by implementing task group number 1:

## Implement this task and its sub-tasks:

### Firmware MQTT Presence
**Assigned implementer:** api-engineer
**Dependencies:** None

- [ ] 1.0 Implement `MqttPresenceClient` that wraps AsyncMqttClient, connects with broker defaults from `include/secrets.h`, and registers retained birth/LWT payloads on `devices/<mac>/state`.
- [ ] 1.1 Derive lowercase MAC topic IDs and build payloads `state=ready ip=<ipv4> msg_id=<uuid>` by pulling identity data from NetOnboarding; ensure LWT publishes `state=offline`.
- [ ] 1.2 Emit presence updates at ~1 Hz and immediately on motion/power state changes without blocking motor tasks; keep firmware operational when broker unreachable and log a single `CTRL:WARN MQTT_CONNECT_FAILED` event.
- [ ] 1.3 Integrate UUID generation (UUIDv4) using a hardware-seeded entropy source, expose it through CommandExecutionContext, and tag every MQTT publication with `msg_id`.
- [ ] 1.4 After presence publishing is validated, migrate serial command acknowledgments to reuse the new UUIDs instead of monotonic CIDs and update related formatting/helpers.
- [ ] 1.5 Update firmware docs/README notes to describe MQTT presence expectations and capture the follow-up task for runtime SET/GET of broker credentials.
- [ ] 1.6 Write 2-4 focused unit tests (native/host where possible) covering payload formatting, UUID propagation, and the broker-failure log path; run only these new tests.

**Acceptance Criteria:**
- Flash firmware, connect to local Mosquitto with defaults, and confirm retained `state=ready ip=<ipv4> msg_id=<uuid>` appears on `devices/<mac>/state`; power-cycle node to see retained `state=offline` and single `CTRL:WARN MQTT_CONNECT_FAILED` when broker is unreachable.
- Run the 2-4 new unit tests and verify they pass.

## Understand the context

Read @agent-os/specs/2025-10-29-mqtt-steel-thread-presence/spec.md to understand the context for this spec and where the current task fits into it.

## Perform the implementation

Implement all tasks assigned to you in your task group.

Focus ONLY on implementing the areas that align with **areas of specialization** (your "areas of specialization" are defined above).

Guide your implementation using:
- **The existing patterns** that you've found and analyzed.
- **User Standards & Preferences** which are defined below.

Self-verify and test your work by:
- Running ONLY the tests you've written (if any) and ensuring those tests pass.
- IF your task involves user-facing UI, and IF you have access to browser testing tools, open a browser and use the feature you've implemented as if you are a user to ensure a user can use the feature in the intended way.


## Update tasks.md task status

In the current spec's `tasks.md` find YOUR task group that's been assigned to YOU and update this task group's parent task and sub-task(s) checked statuses to complete for the specific task(s) that you've implemented.

Mark your task group's parent task and sub-task as complete by changing its checkbox to `- [x]`.

DO NOT update task checkboxes for other task groups that were NOT assigned to you for implementation.


## Document your implementation

Using the task number and task title that's been assigned to you, create a file in the current spec's `implementation` folder called `[task-number]-[task-title]-implementation.md`.

For example, if you've been assigned implement the 3rd task from `tasks.md` and that task's title is "Commenting System", then you must create the file: `agent-os/specs/2025-10-29-mqtt-steel-thread-presence/implementation/3-commenting-system-implementation.md`.

Use the following structure for the content of your implementation documentation:

```markdown
# Task [number]: [Task Title]

## Overview
**Task Reference:** Task #[number] from `agent-os/specs/2025-10-29-mqtt-steel-thread-presence/tasks.md`
**Implemented By:** [Agent Role/Name]
**Date:** [Implementation Date]
**Status:** ‚úÖ Complete | ‚ö†Ô∏è Partial | üîÑ In Progress

### Task Description
[Brief description of what this task was supposed to accomplish]

## Implementation Summary
[High-level overview of the solution implemented - 2-3 short paragraphs explaining the approach taken and why]

## Files Changed/Created

### New Files
- `path/to/file.ext` - [1 short sentence description of purpose]
- `path/to/another/file.ext` - [1 short sentence description of purpose]

### Modified Files
- `path/to/existing/file.ext` - [1 short sentence on what was changed and why]
- `path/to/another/existing/file.ext` - [1 short sentence on what was changed and why]

### Deleted Files
- `path/to/removed/file.ext` - [1 short sentence on why it was removed]

## Key Implementation Details

### [Component/Feature 1]
**Location:** `path/to/file.ext`

[Detailed explanation of this implementation aspect]

**Rationale:** [Why this approach was chosen]

### [Component/Feature 2]
**Location:** `path/to/file.ext`

[Detailed explanation of this implementation aspect]

**Rationale:** [Why this approach was chosen]

## Database Changes (if applicable)

### Migrations
- `[timestamp]_[migration_name].rb` - [What it does]
  - Added tables: [list]
  - Modified tables: [list]
  - Added columns: [list]
  - Added indexes: [list]

### Schema Impact
[Description of how the schema changed and any data implications]

## Dependencies (if applicable)

### New Dependencies Added
- `package-name` (version) - [Purpose/reason for adding]
- `another-package` (version) - [Purpose/reason for adding]

### Configuration Changes
- [Any environment variables, config files, or settings that changed]

## Testing

### Test Files Created/Updated
- `path/to/test/file_spec.rb` - [What is being tested]
- `path/to/feature/test_spec.rb` - [What is being tested]

### Test Coverage
- Unit tests: [‚úÖ Complete | ‚ö†Ô∏è Partial | ‚ùå None]
- Integration tests: [‚úÖ Complete | ‚ö†Ô∏è Partial | ‚ùå None]
- Edge cases covered: [List key edge cases tested]

### Manual Testing Performed
[Description of any manual testing done, including steps to verify the implementation]

## User Standards & Preferences Compliance

In your instructions, you were provided with specific user standards and preferences files under the "User Standards & Preferences Compliance" section. Document how your implementation complies with those standards.

Keep it brief and focus only on the specific standards files that were applicable to your implementation tasks.

For each RELEVANT standards file you were instructed to follow:

### [Standard/Preference File Name]
**File Reference:** `path/to/standards/file.md`

**How Your Implementation Complies:**
[1-2 Sentences to explain specifically how your implementation adheres to the guidelines, patterns, or preferences outlined in this standards file. Include concrete examples from your code.]

**Deviations (if any):**
[If you deviated from any standards in this file, explain what, why, and what the trade-offs were]

---

*Repeat the above structure for each RELEVANT standards file you were instructed to follow*

## Integration Points (if applicable)

### APIs/Endpoints
- `[HTTP Method] /path/to/endpoint` - [Purpose]
  - Request format: [Description]
  - Response format: [Description]

### External Services
- [Any external services or APIs integrated]
```

## User Standards & Preferences Compliance

@agent-os/standards/global/coding-style.md
@agent-os/standards/global/conventions.md
@agent-os/standards/global/platformio-project-setup.md
@agent-os/standards/global/resource-management.md
@agent-os/standards/backend/data-persistence.md
@agent-os/standards/backend/hardware-abstraction.md
@agent-os/standards/backend/task-structure.md
@agent-os/standards/testing/build-validation.md
@agent-os/standards/testing/hardware-validation.md
@agent-os/standards/testing/unit-testing.md
